---
title: "TiBERT: A Non-autoregressive Pre-trained Model for Text Editing"
collection: publications
category: conferences
permalink: /publication/2023-10-08-TiBERT
excerpt: ''
date: 2023-10-08
venue: 'NLPCC 2023'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-031-44699-3_2'
citation: '<b>Baoxin Wang</b>, Ziyue Wang, Wanxiang Che, Dayong Wu, Rui Zhang, Bo Wang, Shijin Wang. Natural Language Processing and Chinese Computing (<b>NLPCC 2023</b>).'
---

Text editing refers to the task of creating new sentences by altering existing text through methods such as replacing, inserting, or deleting. Two commonly used techniques for text editing are Seq2Seq and sequence labeling. The Seq2Seq method can be time-consuming, while the sequence labeling method struggles with multi-token insertion. To solve these issues, we propose a novel pre-trained model called TiBERT, which is specially designed for Text Editing tasks. TiBERT addresses these challenges by adjusting the length of the hidden representation to insert and delete tokens. We pre-train our model using a denoising task on a large dataset. As a result, TiBERT provides not only fast inference but also an improvement in the quality of text generation. We test the model on grammatical error correction, text simplification, and Chinese spelling check tasks. The experimental results show that TiBERT predicts faster and achieves better results than other pre-trained models in these text editing tasks.
